[{"title":"Linux使用","url":"/2022/12/12/Linux%E4%BD%BF%E7%94%A8/","content":"linux的基本使用。\n本地用wsl就可以，在线colab网站也挺好。\n好用命令查找文件。/: 从根目录开始查找。\nfind / -name &quot;local&quot;\n查找进程。grep是搜索，用kill杀死进程。\nps aux | grep bash\n后台运行。nohup , &amp;  , 2&gt;&amp;1 将错误输出重定向到标准输出\nnohup ./clash-linux-amd64-v1.10.0 -f xiaoyin_linux.yml -d . &gt; clash.log 2&gt;&amp;1 &amp;\n 修改权限。\nchmod 777 test.txt \nwhereis\n常用配置更换pip清华源，快的飞起。  有时候换科大源快，可能离得近吧。\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\nWSL2windows开启Linux子系统设置\n微软商店下个20.04ubuntu\n再在vscode中打开，linus编程比windows好很多。\n","tags":["linux"]},{"title":"GPU并行","url":"/2023/11/20/GPU%E5%B9%B6%E8%A1%8C/","content":"简介主要针对transformer大模型架构，megatron-LM框架。\n资料\nNVIDIA/Megatron-LM   官网里面给出了经典的3篇文章\n常见的分布式并行策略 - OneFlow\n并行技术 | Colossal-AI (colossalai.org)\n数据并行DP，要 all-reduce 互相传梯度。\nzero1 对于 optimizer states 的切分。每个gpu只维护一部分优化器状态，也只更新一部分参数。这样显存和计算都减少了。\n\n通信量并没有增加。Ring AllReduce就是分为两个步骤：reduce-scatter和 all-gather。\n通信量：$2\\times (N-1)  \\times  \\frac{K}{N} $    k是数据大小，N个GPU\n资料\nRing AllReduce简介 \n张量并行将attention、MLP拆开。只要在g处做all-reduce加在一起就行。\n\nsequence parallel将 layernorm 和 dropout 按sequence维度也拆分。通信g是前向的all-gather，反向reduce-scatter，$\\hat{g}$相反。同样通信量也没有增加。\n\n流水线并行Gpipe。算完前向再算反向。\n\n1F1B、交错流水线 。上面是1F1B，保持进行中的微批次数量里只有4个（与管道深度一样），这样相比Gpipe显存会更小。\nlayers 16，pp4配置下。1F1B划分：1-4，5-8，9-12，13-16。交错流水线可以1-2，9-10；3-4，11-12；5-6，13-14；7-8，15-16这样划分，可以理解为先把模型划分成几块（2块），再依次划分给GPU。这样需要更多轮1234才能算完（2次）一个前（反）向，但是时间也相应减小了，使得bubble占比更小。\n\nmoe专家并行基于 Transformer 的 MoE，即将 Transformer 中的 MLP 层替换为 MoE 层。其他操作没变。\n从数据并行到专家并行需要转化切分维度，做all-to-all操作。\n在megatron中，是用all-gather、reduce-scatter实现all-to-all功能的。\n\n","tags":["GPU并行"]},{"title":"Megatron-LM实验","url":"/2023/10/19/Megatron-LM%E5%AE%9E%E9%AA%8C/","content":"运行megatron代码注释trans engine模块，参数   —transformer-impl local \n创建环境\ndocker run -it --name ydshi_3_1 --gpus all --shm-size=1G -v /home/ydshi/data:/workspace nvcr.io/nvidia/pytorch:22.04-py3 /bin/bash\ngpt需要下载vocab，merges 2个文件，可以在hugging face下载到。按官网教程预处理数据。将sh脚本补充完整，并自定义好参数，GPUS_PER_NODE、tp、pp等。\n运行命令就可以了。\nbash examples/pretrain_gpt_distributed_with_mp.sh &gt; record.txt \ntipsno.1CUDA out of memory，就是显存不够了，可以减小一些参数。\n​    —num-layers 12 \\​    —hidden-size 256  \n统计nccl通信占比在interation 代码前后插入 torch.cuda.nvtx.range_push   、 …pop tag。运行时前面加上nsys profile，在windows 使用NsightSystems 进行可视化查看。（一般第一个iteration是warm up，可以设置成5、10）\n\n下载并安装师兄改版的pyprof。在megatron-LM/result中\n使用 myrun.sh 将nsys文件转化成 csv表格。在可视化文件中复制iteration的信息（要找default stream），如图。\nName\tStart\tDuration\tTID\tGPU\tContextDescription of the event [968.163 ms]\t18.0755s\t968.163 ms\t117199\tGPU 1\tStream 7\n复制到interation.py, 再改下文件名，运行interation.py这个就行，\n会调用到algo.py ，统计kernel时间的。\nmyrun.shnsys profile bash examples/pretrain_gpt_distributed_with_mp.sh &gt; record.txt nsys stats --report=sqlite  pp4.nsys-reppython3 -m pyprof.parse pp4.sqlite &gt; pp4.jsonpython3 -m pyprof.prof --csv -c idx,kernel,sil,device,stream,start,end,grid,block pp4.json &gt; pp4.csv\n","tags":["Megatron-LM"]},{"title":"Transformer","url":"/2023/11/01/transformer/","content":"embedding参数维度大小是（v,h），v是词表总量，h是hidden_size。 每个词有个整数索引，embedding后就是对应索引的那行，python代码中张量直接套一个索引就得出了。\nself-attention能提取序列的关系，取代了之前的RNN，方便并行。\n由input a1分别算出q，k，v。attention就是每个词之间的关系，q1乘其他词的k得到，最后拿attention乘对应的v 再加起来，就是output b1。要训练的就是Wq,Wk,Wv。\n\nmuti-head就是多个self-attention，可以提取不同角度的信息。\n资料\nTransformer - YouTube   讲的很清楚\n","tags":["transformer"]},{"title":"推理","url":"/2024/04/05/%E6%8E%A8%E7%90%86/","content":"LLM类加载模型时首先根据参数生成llm_engine类\n\nprefiil - decode 2个阶段\nKV-cache 缓存，token -&gt; token 生成下一个token需要当前的Q，和之前所有的KV。\n资料\nLLM推理入门指南②：深入解析KV缓存 (qq.com)\nHow a Transformer works at inference vs training time - YouTube\n"},{"title":"方法记录","url":"/2023/12/26/%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95/","content":"查找新模型PyTorch库 timm有很多SOTA模型 huggingface/pytorch-image-models ，再结合 模型榜单 就能知道最新的模型了。\ntimm 库引用模型并打印结构\nmodel = timm.create_model(&#x27;coatnet_0_224&#x27;)for name, param in model.named_parameters():    print(f&quot;Parameter name: &#123;name&#125;, &#123;param.size()&#125;&quot;)\n","tags":["方法记录"]},{"title":"深度学习基础","url":"/2023/12/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/","content":"常用方法Cross Entropy Loss交叉熵主要用于度量两个概率分布间的差异性。标签值 y（分类时用one-hot编码，真实类别是1，其他为0）， 预测值 a，分类数n\n\n\\text { loss }=-\\sum_{j=1}^n y_j \\ln a_jsoftmax多类别分类的激活函数，将一组实数转化为表示概率分布的值。每个元素都在 0 到 1 之间，且所有元素的和为 1。\n给定输入向量 $z=\\left(z_1, z_2, \\ldots, z_k\\right)$ ，计算公式为:\n\n\\begin{aligned}\n\\sigma(z)_i=\\frac{e^{z_i}}{\\sum_{j=1}^k e^{z_j}}，i=1,2, \\ldots, k\n\\end{aligned}其中， $e$ 是自然对数的底， $k$ 是类别的数量。\nlarynorm\n用于加速训练的。对单个样本的不同特征做归一化操作，公式：\n\n\\begin{aligned}\n\\mu_i & =\\frac{1}{n} \\sum_{j=1}^n x_{i j} \\\\\n\\sigma_i^2 & =\\frac{1}{m} \\sum_{j=1}^m\\left(x_{i j}-\\mu_i\\right)^2 \\\\\n\\hat{x}_{i j} & =\\frac{x_{i j}-\\mu_i}{\\sqrt{\\sigma_i^2+\\epsilon}}\n\\end{aligned}\n\\begin{aligned}\n\n\n \\end{aligned}dropout用于防止神经网络过拟合。在训练过程中，随机地将一部分神经元的输出设置为零。\n资料\n03.2 交叉熵损失函数 - AI-EDU (microsoft.github.io)\n优化器深度学习的优化目标是最小化目标（损失）函数，就是反向传播算梯度从而更新参数。不同优化器区别在于q函数。\n待优化参数: $\\theta$ ，目标函数： $f(\\theta)$ ，学习率: $\\alpha$\n $\\mathrm{t}$ 时刻参数的梯度: $g_t=\\nabla f\\left(\\theta_t\\right)$\n优化通式：  $\\theta_t=\\theta_{t-1}-q\\left(g_t\\right)$\nSGDSGD的梯度下降过程，类似于一个小球从山坡上滚下，它的前进方向只与当前山坡的最大倾斜方向一致(梯度反方向)，每一个时刻的初速度都为０。\n\n\\begin{aligned}\n\n\\theta_t=\\theta_{t-1}-\\alpha * g_t\n\n \\end{aligned}SGD容易收敛到局部最优，在某些情况下可能被困在鞍点。\n引入 Momentum \n\n\\begin{aligned}\n\nm_t & =\\text { momentum } * m_{t-1}+\\alpha * g_t \\\\\n\\theta_t & =\\theta_{t-1}-m_t\n\n\\end{aligned}假设momentum =0.9,$ \\alpha=0.01$, 有:\n\n\\begin{aligned}\n& m_5=0.9 m_4+0.01 g_5 \\\\\n& m_4=0.9 m_3+0.01 g_4 \\\\\n& m_3=0.9 m_2+0.01 g_3 \\\\\n& m_2=0.9 m_1+0.01 g_2\n\\end{aligned}则: $m_5=0.01 *\\left(g_5+0.9 g_5+0.9^2 g_3+0.9^3 g_2+0.9^4 g_1\\right)$可以看到第5次更新的梯度包含了前4次的梯度，且是一个指数衰减的过程。\nSGD Momentum的梯度下降过程，前进方向由当前山坡的最大倾斜方向与之前的下降方向共同决定，小球具有初速度(动量)。\nAdam\n\\begin{aligned}\nm_t & =\\beta_1 m_{t-1}+\\left(1-\\beta_1\\right) g_t \\\\\n\\hat{m}_t & =\\frac{m_t}{1-\\beta_1^t} \\\\\nV_t & =\\beta_2 V_{t-1}+\\left(1-\\beta_2\\right) g_t^2 \\\\\n\\hat{V}_t & =\\frac{V_t}{1-\\beta_2^t} \\\\\n\\alpha_t & =\\frac{\\alpha}{\\sqrt{\\hat{V}_t}+\\epsilon} \\\\\n\\theta_t & =\\theta_{t-1}-\\alpha_t * \\hat{m}_t    \\\\\n\n\\end{aligned}二阶动量$V_t $ 是历史梯度平方和，度量历史更新频率。参数更新越频繁，二阶动量越大，学习率就越小。$\\hat{m}_t 、\\hat{V}_t$是偏差修正。\n资料\noptimizer优化器总结 - weber’s Blog (haiping.vip)\n反向传播就是求损失函数C对参数的梯度，利用梯度更新参数。\n反向计算梯度时，根据前向不同的计算公式，从而有相应的系数。\n资料 \nBack Propagation and Gradient Calculation in Deep Learning (hannlp.github.io)\n下篇 反向传播的微积分原理_哔哩哔哩_bilibili\n卷积神经网络(CNN)反向传播算法 - 刘建平Pinard - 博客园 (cnblogs.com)\nconv2d结构Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) \n这个卷积层参数：$64\\times3\\times9+64 = 1792$  \n一个通道的结果： 输入的3个通道各自经过一个卷积核再相加得到的，卷积核个数=输出通道*输入通道，每个通道还有有一个偏置。 \n资料\n卷积操作参数量和计算量       卷积核图示\n","tags":["深度学习"]},{"title":"硬件知识","url":"/2024/03/13/%E7%A1%AC%E4%BB%B6%E7%9F%A5%E8%AF%86/","content":"NIC 网卡\n随机访问存储器RAM（Random Access Memory） /DDR/ HBM（High Bandwidth Memory） 都是内存\nPCIe switch  就是一个接口拓展，把硬件连接起来\nPCIe总线资料\nPCIe（一） —— 基础概念与设备树 | Soul Orbit (r12f.com)\nRDMA跨机直接访问内存的技术，会减少开销\n资料\n【最新】Nvidia GPU互联技术全景图 - 知乎 (zhihu.com)\n","tags":["硬件"]},{"title":"DDP","url":"/2024/01/03/%E4%BB%A3%E7%A0%81/DDP%20demo/","content":"DDP \ntorch中DDP比DP效果好，用torchrun启动程序。\n还有hugging face 的accelerate。\n","tags":["并行"]},{"title":"Docker使用","url":"/2024/04/09/%E5%AE%8C%E7%BB%93/docker%E4%BD%BF%E7%94%A8/","content":"dockerpython ，pytorch，cuda都要相互兼容。可以用官方提供的docker，如nvidia。\ncuda版本查看 ，其他的如nvidia-smi都不是。\npython -c &quot;import torch; print(torch.version.cuda)&quot;\ndocker run的时候要-v挂载数据， —shm-size扩大空间。docker工作目录命名为workspace，还得重新命名。\ndocker run -it --name ydshi_3_1 --gpus all --shm-size=1G -v /home/ydshi/data:/workspace nvcr.io/nvidia/pytorch:22.04-py3 /bin/bash\n创建完容器之后重新打开\ndocker start ydshi_3_1docker exec -it ydshi_3_1 /bin/bash \n查看镜像        docker images\n查看容器        docker ps \nssh连接需要用户名、ip地址、密码。默认端口22。\nssh username@host\n可以在vscode里面下载ssh插件，打开服务器的文件夹\n心得docker也需要用  conda  来管理环境的，不过默认在用base。\nvscode可直接连接到docker，再选择docker里面的conda python解释器，就可以跳转、查看参数了。\npylance类型检查报错，可以在vscode setting里面关闭。\n\n最后能在服务器里面愉快的编程了。\n","tags":["docker"]},{"title":"情感历程","url":"/2024/05/09/%E7%94%9F%E6%B4%BB-%E6%83%85%E6%84%9F/%E6%83%85%E6%84%9F%E5%8E%86%E7%A8%8B/","content":"\n  463f9e316d1170d91999ca3c1874fb1bafeffb428b0c40ef5707938beebbd7885672e69f00b3c0d7fede85e74c941565825195cd37c966c973f841a82726af660a7db292629f4d85b6bb7ee6d17e669817b4847c0948d6240aad8e6660a181c50641b7a7037088ed23dcc095b578d85c33cb1452f4b8408db4bc6b251a8d28108eb3782865f05b57f77497555d554e91d1008aeb01a0ccd84ed16a3f24e35527e86bb79555334f991e3da9f2d5272c9ccccace542e15aa24c5db44ed82c16243aad7938024b5614a7c6dac13431355871567725464e337bdeb116830f883897bcaeda103c431a3a3d38fa381cee4949f1d05a2464d05c97579c6e6c8eba98eb3efe36dbcf7c1452c9813e529e05510a1a9322bcc53c38e7b34781c94518794699772cebfa7bc12ae2e45deaeea81750c959623d3a295b00acb7fa7269239e3cb6ca492cf8e9251e4acd89c14fb608b48017d8fdb04adbb9daa0f0c58af8249fc7bdf34c5a9ba236d0144aad8af66e5403483a131fc5bd1307fb058c77fd36bc1754513a7dcbbf871bf89795fe2c62e90aa5b69dd7a594b6e34acb91c9220a278ffdbec7a04fc39a3a14f5c33c6289e9783daa1353271606ec4ab5453c153f066562f967694ed92b01b2eff25214f7d69\n  \n    \n      \n      \n        请输入密码后查看\n      \n    \n  \n\n","tags":["private","生活"]},{"title":"芙莉莲","url":"/2024/05/09/%E7%94%9F%E6%B4%BB-%E6%83%85%E6%84%9F/%E8%8A%99%E8%8E%89%E8%8E%B2/","content":"我人是很需要被肯定的。\n需要被赞赏，\n我很喜欢这种风格的动漫。\n","tags":["生活"]},{"title":"博客使用","url":"/2022/09/02/%E5%AE%8C%E7%BB%93/hexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8/","content":"hexo+keep主题的静态网站。\n我需要的博客功能：    \nA：代码折叠，数学公式，简洁美观。\n文章不太想公开：\nA：用密码访问，在github仓库能看到。Hexo加密   \n还能放在_draft文件夹不上传，在本地先看着，这个别人完全看不到。\nGit Bash进入博客目录中（D:\\blog，D:\\blog\\source\\_posts都行）。\n常用命令：\nhexo clean  # 清除缓存hexo g      # 生成静态网页hexo s      #启动服务器，本地查看hexo d      # 部署到Github\n图床使用PicGO在github上创建图床，picgo要打开时间戳重命名。\n还要下载compress插件，搭配压缩网站（tinify.com)  。\ntypora平时就复制图片到统一文件夹如typora/img,是博客的文档就手动将图片上传。本地要发给别人也简单，用python脚本将图片移到同一文件夹，改下引用格式就行。\n数学公式如何在 hexo 中支持 Mathjax  这个实测简单好使\ntips1修改仓库,这样hexo d好使点\nrepository: git@github.com:shiyandong/shiyandong.github.io.git#repository: https://github.com/shiyandong/shiyandong.github.io.git\ntips2默认的git bash的框太丑了，也不能缩放。得把git bash加到windows终端里面。\n","tags":["blog"]}]